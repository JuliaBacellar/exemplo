from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import CTransformers
from langchain.prompts import PromptTemplate
from langchain_community.cache import InMemoryCache
from langchain.globals import set_llm_cache
import os
import time
import traceback

# Configura√ß√µes globais
CONFIG = {
    "model_path": "modelo/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    "pdf_path": "data/exemplo.pdf",
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    "faiss_index": "faiss_index",
    "llm_config": {
        'max_new_tokens': 150,
        'temperature': 0.7,
        'gpu_layers': 40 if os.environ.get("CUDA_VISIBLE_DEVICES") else 0,
        'batch_size': 8,
        'threads': min(4, os.cpu_count()),
        'context_length': 2048
    }
}

def inicializar_sistema():
    """Configura cache e verifica recursos"""
    set_llm_cache(InMemoryCache())
    print(f"üñ•Ô∏è Threads dispon√≠veis: {CONFIG['llm_config']['threads']}")
    print(f"üèóÔ∏è Configura√ß√£o GPU: {'Ativada' if CONFIG['llm_config']['gpu_layers'] else 'Desativada'}")

def carregar_documentos():
    """Carrega e divide o documento PDF"""
    print("‚è≥ Carregando documento...")
    start = time.time()
    
    if not os.path.exists(CONFIG["pdf_path"]):
        raise FileNotFoundError(f"Arquivo PDF n√£o encontrado em {CONFIG['pdf_path']}")

    loader = PyPDFLoader(CONFIG["pdf_path"])
    documentos = loader.load()
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=64,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    documentos_split = splitter.split_documents(documentos)
    print(f"‚úÖ Documento carregado em {time.time() - start:.2f}s")
    print(f"üìÑ P√°ginas: {len(documentos)} | Chunks: {len(documentos_split)}")
    return documentos_split

def inicializar_vectorstore(documentos):
    """Inicializa ou carrega o vectorstore FAISS"""
    print("‚è≥ Preparando vetores...")
    start = time.time()
    
    embeddings = HuggingFaceEmbeddings(
        model_name=CONFIG["embedding_model"],
        model_kwargs={'device': 'cuda' if CONFIG['llm_config']['gpu_layers'] else 'cpu'}
    )
    
    if os.path.exists(CONFIG["faiss_index"]):
        vectorstore = FAISS.load_local(
            CONFIG["faiss_index"], 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        print(f"üîç Vectorstore carregado: {vectorstore.index.ntotal} vetores")
    else:
        vectorstore = FAISS.from_documents(documentos, embeddings)
        vectorstore.save_local(CONFIG["faiss_index"])
        print("üÜï Novo vectorstore criado")
    
    print(f"‚úÖ Vetores prontos em {time.time() - start:.2f}s")
    return vectorstore

def criar_llm():
    """Configura o modelo de linguagem"""
    print("üî• Aquecendo o modelo LLM...")
    start = time.time()
    
    llm = CTransformers(
        model=CONFIG["model_path"],
        model_type="llama",
        config=CONFIG["llm_config"]
    )
    
    # Teste inicial
    llm.invoke("Warming up")
    print(f"‚úÖ Modelo pronto em {time.time() - start:.2f}s")
    return llm

def criar_chain(llm, retriever):
    """Configura a cadeia de QA"""
    template = """Voc√™ √© um assistente AI √∫til. Responda em portugu√™s de forma clara e concisa.

Contexto: {context}
Pergunta: {question}

Resposta:"""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=False
    )


def processar_pergunta(qa_chain, pergunta):
    """Processa uma pergunta e exibe o resultado"""
    print("‚è≥ Processando...")
    start = time.time()
    
    try:
        resposta = qa_chain.invoke({"question": pergunta})
        tempo_resposta = time.time() - start
        print(f"\nüß† Resposta ({tempo_resposta:.2f}s):")
        print(resposta["result"].strip() + "\n")
    except Exception as e:
        print("\n‚ö†Ô∏è Erro na gera√ß√£o da resposta:")
        traceback.print_exc()
        print(f"\nDica: Tente reformular sua pergunta.\n")

def main():
    """Fun√ß√£o principal do sistema de QA"""
    print("\n" + "="*50)
    print("üîÑ Iniciando Sistema de Perguntas e Respostas")
    print("="*50 + "\n")
    
    try:
        inicializar_sistema()
        documentos = carregar_documentos()
        vectorstore = inicializar_vectorstore(documentos)
        llm = criar_llm()
        
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 2}
        )
        
        qa_chain = criar_chain(llm, retriever)
        
        print("\n" + "="*50)
        print("‚úÖ Sistema pronto! Digite sua pergunta ou 'sair' para encerrar")
        print("="*50 + "\n")
        
        while True:
            pergunta = input("‚ùì Pergunta: ").strip()
            if pergunta.lower() in ('sair', 'exit', 'quit'):
                break
                
            if not pergunta:
                print("‚ö†Ô∏è Por favor, digite uma pergunta v√°lida.\n")
                continue
                
            processar_pergunta(qa_chain, pergunta)
            
    except Exception as e:
        print("\n‚ùå Erro cr√≠tico durante a inicializa√ß√£o:")
        traceback.print_exc()
        print("\nSugest√µes:")
        print("- Verifique se o arquivo PDF existe")
        print("- Confira o caminho do modelo LLM")
        print("- Verifique os recursos dispon√≠veis (GPU/RAM)\n")
    finally:
        print("\nüî¥ Sistema encerrado. At√© logo!\n")

if __name__ == "__main__":
    main()